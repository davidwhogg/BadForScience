% To do:
% ------
% - Write an outline.
% - Fill in the outline.
% - Publish.
% - Wait for the call from Stockholm.

% Style notes:
% ------------
% - Uhhh.

\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage[letterpaper]{geometry}

% Hogg typesetting issues
\setlength{\textwidth}{6.50in}
\setlength{\oddsidemargin}{0.00in}
\setlength{\textheight}{9.00in}
\setlength{\topmargin}{-0.50in}
\setlength{\parindent}{1.2\baselineskip} % seriously
\linespread{1.08}
\frenchspacing\raggedbottom\sloppy\sloppypar
\renewcommand{\author}[1]{\medskip\par\noindent\textbf{#1}}
\newcommand{\affil}[1]{{\footnotesize\par\noindent #1 \par}}

% Gross
\newcommand{\apj}{The Astrophysical Journal}
\newcommand{\araa}{Annual Reviews in Astronomy and Astrophysics}

% Math issues
\newcommand{\teff}{T_{\mathrm{eff}}}
\newcommand{\logg}{\log g}
\newcommand{\feh}{[\mathrm{Fe}/\mathrm{H}]}

\begin{document}

\section*{Is machine learning good or bad for the natural sciences?}

\author{David W. Hogg}
\affil{Center for Cosmology and Particle Physics, Department of Physics, New York University, 726~Broadway, New~York,~NY 10003, USA}
\affil{Center for Data Science, New York University, 60 Fifth Avenue, New York, NY 10011, USA}
\affil{Max-Planck-Institut f{\"u}r Astronomie, K{\"o}nigstuhl 17, D-69117 Heidelberg, Germany}
\affil{Flatiron Institute, a division of the Simons Foundation, 162 Fifth Avenue, New~York,~NY 10010, USA}

\paragraph{Abstract}
  Machine learning (ML) methods are having a huge impact across all of the sciences.
  Advances have come from classification, regression, dimensionality reduction, clustering, emulation, and many other things.
  However, ML has a strong ontology---in which only the data exist---and a strong epistemology---in which a model is considered good if it performs well on held-out training data.
  These philosophies are in strong conflict with both standard practices and key philosophies in the natural sciences.
  Here I identify locations for ML in the natural sciences where the ontology and epistemology (and computational performance) are valuable.
  I also call out uses for ML that are not a good idea.
  In particular, there are a lot of reasons to be concerned about the use of complex ML models to emulate physical (or first-principles) simulations, for epistemological and confirmation-bias reasons.
  On the other hand, there are also places where the introduction of ML can in fact make a project more scientifically conservative:
  When a flexible machine-learning model is used to represent the effects of nuisances, such as foregrounds, backgrounds, or instrument calibration parameters, the model capacity and loose philosophy of the method can sometimes make the results more trustworthy.
  These remarks are intended to apply to all of the natural sciences, but most of the specific examples are in the astrophysics domain.

\section{Introduction}\label{sec:intro}

It is an understatement to say that deep learning is having a big impact across the sciences.
There are hundreds of papers per year now on deep-learning methods in astrophysics alone.
However, when we ask what scientific breakthroughs have been enabled by this influx of new tools and methods, there isn't a long list.
The success of the AlphaFold projects in protein structure (\cite{alphafold}) are often raised.
But these are successes in a biological or medical field, not astrophysics, and in a very specific challenge-problem setting in which performance is valued over understanding.
Here in astrophysics we care almost exclusively about understanding.

If we broaden our scope from deep learning to machine learning more generally, we can point to very important contributions of machine learning to astrophysics.
Kepler discoveries
PCA and, eg, SDSS redshift system

Astrophysics is an academic discipline, concerned with understanding the physical world, and naturally occurring mechanisms in play in that world.
We make progress by discovering new kinds of objects and phenomena, and explaining (and, even better, predicting) new kinds of objects and phenomena.
Our most successful investigations are judged in terms of the questions they answer, or the new questions they raise, or both.
The question here is: How can deep learning contribute to this mission?

I mentioned the word ``performance'' above.
One of the key ideas of machine-learning methods is that they are usually judged in terms of their performance, or their ability to predict or explain new data.
That's different, philosophically, from what's traditionally important in astrophysics.
When the expansion of the Universe was discovered (\cite{expansion, expansion2}), the result was not (primarily) judged on the basis of its ability to predict new data.
That is, the investigators were not mainly trying to predict the Doppler shifts of held-out galaxies!
They were trying to measure a property of the Universe, traced by galaxies.
And their result answered some important questions, and raised new ones.
Of course the expansion result has lasted in part because it \emph{does} make new predictions for new data, and very accurate predictions!
So it isn't irrelevant that the performance of the hypothesis is good.
However, there is some disconnect between how we assess a machine-learning method and how we assess a scientific contribution.

HOGG: NEED to make the point, here or below, that I respect the ML point of view. It could be helpful for astrophysics if we moved towards it in some ways. Oh and the statistical care is of interest too. Maybe this is a section about the underlying philosophies of ML.

I don't want to sound anti-technological however.
Machine learning is a new and productive technology.
We should use it!
Astrophysics has moved forward through technological progress, starting with Galileo (\cite{galileo}).
In my own lifetime, the most remarkable technological advance was (probably) the change from photographic plates to charge-coupled devices (CCDs; see, for an early review, \cite{ccd}).
In just a few years, almost every telescope and spectrograph in the world switched from a photographic focal plane to a CCD focal plane, and immediately there were new discoveries.
That's a case in which new technology immediately translated into new discoveries.
That hasn't been nearly so true with machine learning, and (I note with respect), it is way easier to incorporate machine learning into your workflow than to incorporate CCDs into your optical path.

I am both a developer of machine-learning methods, and a skeptic who thinks they should only rarely be used.
That said, there are reasons that we absolutely must use machine learning in the near future of astrophysics.
And there are indeed successes of machine learning that might seem invisible.
I'll try to speak to both of these points below.

\section{What is machine learning?}\label{sec:what}

There is a vague definition of machine learning in terms of data and capacity:
A method is a machine-learning method if its capability increases (hopefully dramatically) as the machine sees more data (HOGG GET THIS MORE PRECISE AND CITE).
In some sense this is true of any measurement process:
The measurement improves as more data come in!
So if we are going to be more specific, we'd like the model capacity or capability to improve faster (in some sense) than something like the square root of the increment in data.

There is a more specific definition of machine learning in terms of specific methods:
Whenever a neural network or a Gaussian process is in play, machine learning is in play.
But when the former definition is too broad, the latter is almost always too narrow.
For example, principal components analysis (PCA) is a workhorse in astronomy since way back (for example, \cite{pcaredshift}), and PCA is an old, and simple, but definitely machine-learning, method.

Supervised and unsupervised.

Classification, regression.

Dimensionality reduction, clustering, density estimation.

Something about how we will use the word ``model'' in what follows.

\section{Why do we need machine learning in the natural sciences?}

Label transfer.

Speeding up decisions.

Modeling nuisances.

Classification?

Speeding up simulations?

The short summary is that we need machine learning; we can't live without it.

\section{The ontology and epistemology of machine learning}

HOGG: Definitions of latent variables and so on; what are latent variables and why are they important?

Contemporary machine-learning models---and especially deep-learning models---have explicit unbroken, combinatorically large degeneracies.
That is, the weights and nodes in various layers of the network have permutation symmetries such that a very large space of permutation operators can be applied to the latent space without making any changes whatsoever to the input--output relationships of the model.
That shows that there is no belief in the meaning of any internal latent variables.

Furthermore, stochastic gradient, early stopping and so on:
There isn't even a belief that the latents should be the actual argmin of anything.
The latent values are of absolutely no importance or meaning whatsoever.

If there is any stability to a model, it is the stability of its predictions, or its behavior on held-out data.

Also maybe mention here the statistical philosophy or care.

\section{When is machine learning a bad idea?}

Adversarial attacks argument. Confirmation bias argument. Planet search argument.

\section{When is machine learning the conservative choice?}

Come back to the planet-search example.

\section{Discussion}\label{sec:discussion}

Hello World!

\bibliography{ml_in_astro}
\bibliographystyle{plain}

\end{document}
