% To do:
% ------
% - Change it to be about PHYSICS rather than natural science?
% - Get feedback from BS, MSV, JH, others
% - Update appendix with feedback from Vedant Chandra (email).
% - put in all references
% - audit bibliography; make sure arXiv references in the reference list are consistent? Also capitalization!
% - Publish on the arXiv.
% - set formatting and anonymization switches to go between arXiv and ICML Position Paper
% - submit to ICML.
% - Wait for the call from Stockholm.

% Style notes:
% ------------
% - Be very careful about the SUBJECT of sentences. Is it ML that is bad? Is it the practice that is bad? Is it the practitioner that is bad?
% - Should we use NS for natural science?
% - How many authors are there? Should we switch to first-person singular?
% - Audit for flexible, expressive, high in capacity, and so on.
% - [more here]

\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage[letterpaper]{geometry}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{url}

% Hogg typesetting issues that won't conflict with ICML
\frenchspacing
\newcommand{\documentname}{\textsl{Position Paper}}
\newcommand{\draftstatus}{\textcolor{red}{Draft version 2023-12-31}}
\newcommand{\sectionname}{Section}
\newcommand{\secref}[1]{\sectionname~\ref{#1}}
\newcommand{\figref}[1]{\figurename~\ref{#1}}
\newenvironment{hoggnumerate}
  {\begin{list}
    {\arabic{enumii}.}
    {\usecounter{enumii}
     %\setlength{\labelwidth}{3em}
     %\setlength{\labelsep}{0em}
     \setlength{\itemsep}{0in}
     \setlength{\parsep}{0in}
     \setlength{\parskip}{0in}
     %\setlength{\leftmargin}{1.5cm}
     %\setlength{\rightmargin}{2cm}
     %\setlength{\itemindent}{0em} 
     %\let\makelabel=\makeboxlabel
    }
  }
{\end{list}}

% Hogg typesetting issues that will conflict with ICML
\setlength{\textwidth}{5.50in}
\setlength{\oddsidemargin}{-0.25in}
\setlength{\textheight}{9.25in}
\setlength{\topmargin}{-0.50in}
\setlength{\parindent}{1.2\baselineskip} % seriously
\renewcommand{\title}[1]{\section*{\raggedright #1}}
\renewcommand{\author}[1]{\medskip\par\noindent\textbf{#1}}
\newcommand{\affil}[1]{{\footnotesize\par\noindent #1 \par}}
\renewenvironment{abstract}{\paragraph{Abstract:}}{}
\pagestyle{myheadings}
\markboth{foo}{\textcolor{gray}{\textsf{Hogg / Is machine learning good or bad for the natural sciences? \draftstatus}}}
\linespread{1.08}
\raggedbottom\sloppy\sloppypar

% Gross
\newcommand{\apj}{The Astrophysical Journal}
\newcommand{\araa}{Annual Reviews in Astronomy and Astrophysics}

% Math issues
\newcommand{\teff}{T_{\mathrm{eff}}}
\newcommand{\logg}{\log g}
\newcommand{\feh}{[\mathrm{Fe}/\mathrm{H}]}

\begin{document}\thispagestyle{empty}

\title{Is machine learning good or bad for the natural sciences?}
\noindent
\draftstatus

\author{David W. Hogg}
\affil{Center for Cosmology and Particle Physics, Department of Physics, New York University, 726~Broadway, New~York,~NY 10003, USA}
\affil{Center for Data Science, New York University, 60 Fifth Avenue, New York, NY 10011, USA}
\affil{Max-Planck-Institut f{\"u}r Astronomie, K{\"o}nigstuhl 17, D-69117 Heidelberg, Germany}
\affil{Flatiron Institute, a division of the Simons Foundation, 162 Fifth Avenue, New~York,~NY 10010, USA}

\begin{abstract}
  Machine learning (ML) methods are having a huge impact across all of the sciences.
  However, ML has a strong ontology---in which only the data exist---and a strong epistemology---in which a model is considered good if it performs well on held-out training data.
  These philosophies are in strong conflict with both standard practices and key philosophies in the natural sciences.
  Here we identify locations for ML in the natural sciences where the ontology and epistemology are valuable.
  For example, when an expressive machine-learning model is used in a causal inference to represent the effects of confounders, such as foregrounds, backgrounds, or instrument calibration parameters, the model capacity and loose philosophy of ML can sometimes make the results more trustworthy.
  We also show that the there are contexts in which the introduction of ML introduces strong, unwanted statistical biases.
  For one, when ML models are used to emulate physical (or first-principles) simulations, they introduce strong confirmation biases.
  For another, when expressive regressions are used to label datasets, those labels cannot be used in downstream joint analyses without taking on uncontrolled biases.
  These remarks are intended to apply to all of the natural sciences, but most of the specific examples are in the astrophysics domain.
\end{abstract}

\section{Introduction}\label{sec:intro}
It is an understatement to say that machine learning (ML) is having a big impact across the sciences.
A significant fraction of all scientific papers in the natural sciences now employ ML in part (or all) of their analyses.
However, when we ask what scientific breakthroughs have been enabled by this influx of new tools and methods, there isn't a long list.
The success of the AlphaFold projects in protein structure \cite{alphafold} are often raised.
But these are successes in a very specific challenge-problem setting in which \emph{performance} is valued over \emph{understanding}.
In the natural sciences we almost exclusively care about understanding, in the long run.

The natural sciences are concerned with understanding the world, and naturally occurring mechanisms in play in that world.
We make progress by discovering new kinds of objects and phenomena, and explaining (and, even better, predicting) qualitatively new kinds of objects and phenomena.
Our most successful investigations are judged in terms of the questions they answer, or the new questions they raise, or both.
The question here is: How will ML contribute to this mission?

In contrast to natural science, ML research and ML methods are concerned with making accurate predictions for, or descriptions of, \emph{data}.
A ML method is considered successful if it performs well on held-out training data, even if the latent structure of the model is generic and the internals are impossible to interpret.
In ML, the considerations are all at the level of the data, and we are happy to use models where we have litlle or no understanding of the meanings or values of the latent parameters or weights.
Another way to ask our question is:
\emph{Where, in the natural sciences, can you use a model that you don't understand?}

In natural science the most important contributions and results are all at the level of the \emph{latents}:
We use data to learn about the latent structure of the world or of the system we are studying.
In astrophysics, this could be the interior structure of the Sun, or the statistics of planets around other stars, or the map of the dark matter surrounding the Milky Way.
The things we care about are almost never directly observable; they are parameters (or hyper-parameters) of a physical (or chemical or biological) model that predicts the observables.

For a concrete example, when the expansion of the Universe was discovered \cite{expansion, expansion2}, the discovery was important, but not because it permitted us to predict the values of the redshifts of new galaxies (though it did indeed permit that).
The discovery was important because it told us previously unknown things about the age and evolution of the Universe, and it confirmed a prediction of general relativity, which is a theory of the latent structure of space and time.
The discovery would not have been seen as important if Hubble and Humason had instead announced that they had trained a deep multilayer perceptron that could predict the Doppler shifts of held-out extragalactic nebulae.

We are both ML \emph{skeptics} and ML \emph{practitioners}.
We are writing this for an audience that is either developing an ML method for science applications or else bringing ML into a scientific domain.
The main points of this \documentname{} are \textsl{(1)}~that ML has places where it is very valuable in the contemporary practice of science, and \textsl{(2)}~that it also has places where it will create problems for science.
The answer to the question in the title is ``Both.''

\paragraph{Our contributions:}
\begin{itemize}
  \item We deliver a description of the fundamental \emph{ontology} and \emph{epistemology} of machine learning, and contrast these with the ontologies and epistemologies of the natural sciences.
  \item We elucidate two important and strong statistical biases that are being introduced to the natural sciences by some of the uses of ML in those disciplines. One is a confirmation bias that arises when simulations are replaced or augmented by emulators. Another is a more standard estimator bias that is (possibly enormously) amplified when elements of datasets produced by ML regressions are used in combination or joint analyses. These biases can't be easily corrected.
  \item We show that there are many safe places for the use of ML methods in current natural-science practices, and places where (in the contemporary context) the use of ML methods is effectively \emph{required}. Many of these places are in the operational parts of scientific projects.
  \item We argue that in causal contexts, in which the ML method is used to model foregrounds, backgrounds, calibration parameters, or other confounders, using the most expressive ML method can deliver \emph{the most conservative approach} to the scientific problem.
\end{itemize}

\section{The ontology and epistemology of machine learning}\label{sec:philosophy}

\paragraph{What is machine learning?}
For the purposes of this \documentname, we will employ an expansive or inclusive definition of ML.
For us, a method is ML if its \emph{capability increases substantially as it sees more data} \cite{ml_definition}.
In some sense this definition could be seen as true of any measurement process, since measurements improve as the data improve. 
If we are going to be more specific, we'd like the model precision or capability to improve faster (in some sense) than (something like) the square root of the increment in data.

This definition is broad:
In addition to methods like convolutional neural networks \cite{cnn}, multi-layer perceptrons \cite{mlp}, and transformers \cite{transformer}, it includes large linear regressions \cite{linearregression}, gaussian processes \cite{gp}, support-vector machines \cite{svm}, principal components analysis \cite{pca}, kernel density estimates \cite{kde}, and even some kinds of multilayer or hierarchical models \cite{multilevel}.
Feel free to personally take any more restrictive definition.
Our comments will apply to everything in the larger class.

\paragraph{What is natural science?}
For the purposes of this \documentname, we will employ a restrictive definition of natural science.
For us, the natural sciences are the sciences that study the natural world, with the primary aim of \emph{understanding}.
Examples include physics, chemistry, biology, Earth science, ecology, and so on.
For our purposes, the natural sciences are about understanding observed phenomena, unifying knowledge, making predictions for new experiments and observations.
Natural-science questions include things like: How did our Solar System form? Or how do cells differentiate as the embryo grows? Or what causes the jet stream?

For the purposes of this \documentname{} we will exclude more engineering-oriented questions such as: What protein sequences might be important for the treatment of diabetes? Or where in Canada might there be new uranium deposits? Or what kinds of catalytic surfaces might help turn biomass into biofuel?
This is a bit unfair!
We are excluding these kinds of questions in part because it is obvious that ML methods \emph{can} serve important purposes in such problems.
That will re-emerge as a theme, below.

\paragraph{Ontologies:}
Unsupervised ML methods deliver descriptions or compressions of data.
Supervised ML methods find relationships between data (features) and data (labels).
In both cases, the methods are judged on their capability to accurately describe the data.
They are not judged on the details of their latent structure.
In a very important sense, the ML ontology is that \emph{only the data exist}.

In support of this point of view, here are some comments:
Deep-learning models have enormously large internal degeneracies (combinatorial degeneracies even).
These degeneracies are not seen as a problem, since all they do is make the latent weights less interpretable.
Contemporary optimization schemes are stochastic \cite{stochastic} and most models are non-convex.
The fact that it is impossible to find the global optimum---and the fact that in most contemporary models that isn't even the goal \cite{not_global}---shows that the latent parameters are not important.
The use of early stopping \cite{early_stop} and drop-outs \cite{dropout} to stabilize optimization strengthens this point.
Any practitioner using a 40-layer network can trivially (and without analysis) switch to a 42-layer, despite the fact that (from a functional point of view), this substantially changes the model capacity.

In ML, models are deemed stably optimized and useful not if the latent space---the joint values of all the weights and biases---is stable, but rather if the predictions for held-out data are stable.
Thus it is the data that exist in ML contexts, not the latent parameters.

In contrast, the ontologies of the natural sciences contain far more things than just the data.
In physics, for example, 
not only do the data exist, but so do forces, energies, momenta, charges, spacetime, wave functions, virtual particles, and much more.
These entities are judged to exist in part because they are involved in the latent structure of the successful theories; almost none of them are direct observables.
The most important discoveries in the natural sciences are discoveries of latent structure: Natural selection as an explanation for the differentiated properties of species \cite{natural_selection}, for example, or the elemental composition of the Sun \cite{sun_composition}, or the quarks and gluons that make up the proton \cite{proton_substructure}.

\paragraph{Epistemologies:}
A trained ML model is deemed successful or correct if \emph{it performs well on held-out training data}.
This epistemological position is related strongly to the ontological position:
If only the data exist, then the success of a model is judged only in terms of the data it describes.

In support of this position we could point to the literature on adversarial attacks (eg, \cite{adversarial1}).
This literature shows us, in dramatic ways, that ML methods are not doing what we (na\"ively) believe that humans or scientists or scientific theories do in comparable circumstances.
And yet, these attacks do not suggest (to most practitioners of ML) that the methods are wrong or require revision.
Even the responses to attacks have responded in ways that involve augmentation of data \cite{adversarial_training}, such that the vulnerability to attack is lessened without compromising the fundamental epistemological point that performance on data is the primary standard of truth.

A critic of our ML epistemology claim could point to the large literature on out-of-sample generalization, transfer learning \cite{transfer}, or the more grand contemporary idea of foundation models \cite{foundation}.
But even in these contexts, models are deemed successful when they explain new or held-out data; the latent structure of the models is not a primary consideration when the validity of the model is in question.

Another critic might point to the literatures on interpretable ML \cite{interpretable} and explainable ML \cite{explainable}.
In these ML subfields, in some cases, attempts are made to understand the internal structure or the behavior of the ML model.
There are not enormous successes here, but if there were, these might be subfields in which the epistemology of ML gets more complex; maybe in the future some ML models will have to perform well on data \emph{and} be explainable in some important sense.
At present this isn't part of any ML epistemology.

In the natural sciences, in contrast, the epistemologies are much more restrictive and much more demanding than they are in ML.
The detailed epistemological framework depends on the natural-science field, and even on the practitioner within the field.
However, in all cases, it is more demainding:
A theory or explanation has to do much more than just explain the data in order to be accepted as true.
In physics for example, a model---which, as we note, is almost always a model of latent structure---is judged to be good or strongly confirmed not just if it explains observed data.
It must explain data in multiple domains, and it must also connect in natural ways to other theories or principles (such as conservation laws and invariances) that are strongly confirmed themselves.
(It is worthy of note here that a successful theory in the natural sciences is usually a combination of a more fundamental theory of the latents, and a less fundamental or auxiliary observation model, which explains how the latents appear in or affect the observable data \cite{auxiliary}.)
General relativity \cite{gr} was adopted not primarily because it explained anomalous data (although it did explain some); it was adopted because it had a beautiful structure, it resolved conceptual paradoxes in the pre-existing theory of gravity, and it was consistent with emerging ideas of field theory and geometry.

\section{Why do we need machine learning in the natural sciences?}\label{sec:why}
In this \sectionname{} we give some idea of how and why ML has had such a big impact in the natural sciences, and how and why many scientific projects require ML technologies.
In most cases, the ML is required on the engineering or execution side of projects that are large-scale in some sense.
\begin{description}
  \item[Label transfer:] Sometimes a project has informative data on a very large number of objects, but precise labels for only a few, maybe obtained through very careful analysis or external data.
  In this case, if it is extremely expensive to label more objects, a regression can be trained on the few labeled data points and then the trained regression used to label all the rest.
  Below (\secref{sec:bad}) we argue that this is not an advisable use of ML in the natural sciences if the expectation is that the regression-generated labels are going to be used in downstream scientific projects.
  \item[Classification:] This is the same use case as label transfer, but in the specific case in which the labels are drawn from a finite (small, actually) set of discrete values.
  It is subject to the same kinds of biases described below (\secref{sec:bad}).
  \item[Speeding up decisions:] Many scientific projects must make decisions very fast in real time.
  The most extreme examples of this are in particle physics, where detectors (such as the \textsl{Atlas} Experiment \cite{atlas} at the \textsl{Large Hadron Collider}) must decide whether to trigger a data-saving event in a tiny fraction of second.
  It is often the case that trained ML classifiers can reproduce the selection boundaries as well---or nearly as well---as first-principles models, but with far less computation.
  It is critical, if ML methods are used for real-time operations, that the methods and all their latent parameters (weights) be preserved for analysis, operations simulations, and conterfactual exercises, often performed long after the data are taken.
  \item[Speeding up simulations:] In much of the natural sciences, the theory in play is a computation or simulation or digital twin.
  These simulations tend to be very computationally expensive, since they often span large ranges of spatial or time scales.
  ML regressions can be trained to emulate the simulations, or patch up low-resolution simulations to higher resolutions.
  Below (\secref{sec:bad}) we argue that the introduction of ML emulators can introduce an unwanted, strong confirmation bias.
  \item[Modeling nuisances:] In most natural-science domains, the quantities of interest are not directly observable, but rather the model or prediction for the data is a combination of the theory of relevance plus auxilliary theories of foregrounds, backgrounds, instrumentation, and noise sources.
  When the goal is for these nuisances to be effectively modeled but not necessarily understood in detail, ML approaches can be effective and (as we argue in \secref{sec:good}) even conservative.
  \item[Outlier detection:] In many of the sciences, discovery of new or previously unknown phenomena or objects can be of great importance.
  A recent example in astronomy is the discovery of fast radio bursts \cite{grbs}, which started as nuisances in imaging projects and have turned out to be interesting astrophysical objects.
  Since expressive unsupervised ML methods can describe accurately a complex distribution of data, they can also be used to find rare data points that are unlike any elements of the training data.
  \item[Making discoveries?] There is a new hope in the sciences that sufficiently trained or constrained models might lead to new insights about scientific theories or might effectively make fundamental discoveries.
  There are approaches along the direction of symbolic regression \cite{symbolic1, symbolic2}, and there are approaches along the lines of foundation models \cite{foundation1, foundation2}.
  So far this hope has borne no important new discoveries, but discoveries are not inconceivable.
\end{description}
This list isn't exhaustive!
For example, it doesn't mention the use of ML to find summary statistics for the comparison of simulations and data in (say) simulation-based inferences \cite{sbi}.
It doesn't mention using ML to emulate likelihoods or likelihood ratios \cite{biwei, likelihood_ratio}.
The uses of ML in the natural sciences is legion.

The short summary of all this is that we need ML in the natural sciences; we can't live without it.
The question here is---given the restrictive ontology and loose epistemology---how we will use it safely?

\section{When is machine learning bad for natural science?}\label{sec:bad}
In this \sectionname{} we elucidate two statistical biases that can be introduced into a natural-sciences project when ML methods are introduced.
Neither of these biases can be easily corrected or removed, to our knowledge.

\paragraph{Amplifications of training-set biases:}
The outcomes of ML regressions are label estimates that are conditioned on the input features \emph{and also} on the totality of the training set used to train the regression.
This is good; the individual-data-point label estimates from the regression are the lowest ``risk'' (in the statistical sense) when they use non-zero bias in the bias--variance trade-off.
However, the biases that are weak or manageable on an individual data-point basis become strong or unacceptable biases when outputs of regressions are used jointly or in combination to measure a population or sub-population property.

This problem is demonstrated with a toy example in \appendixname~\ref{app:toy}.
It is worst when the regression is used to label a large data set or catalog or sample of data,
and when elements of that data set are used in populations or joint analyses.
In general, when multiple data point estimates are used jointly, the variance-induced offsets of the estimators average out but the bias-induced offsets remain fixed.
This is a straightforward point of statistics---this is not news---but it isn't currently informing most of the practice of ML in the natural sciences.
For example, HOGG CITES.

In principle, there are fixes for this problem that involve de-biasing the estimates.
This is not possible in general, because there is not usually enough validation data to accurately assess the bias.

In Bayesian language, this problem is closely related to the point that when data are to be combined, they should be combined at the likelihood level, not the posterior level.
Likelihoods are not affected by population-level biases.
Likelihoods can produce unbiased point estimates.
Likelihood-based estimators are not usually the lowest-risk estimators, but they can be low- or zero-bias estimates.
The outputs of regressions are more like posterior-based estimators, affected by the implicit prior set by the training set.
When they are combined, the implicit prior gets amplified.
Most ML regressions are not capable of generating or emulating likelihoods.
There are interesting approaches in development to replace standard regressions with generative models that can produce likelihoods \cite{cannon, biwei, likelihood_ratio}.

Finally we note---and demonstrate in \appendixname~\ref{app:toy}---that none of these problems arise from out-of-sample generalization or distribution-shift problems.
Distribution shifts just tend to make these kinds of problems worse.

\paragraph{Emulator-induced confirmation bias:}
In many fields in the natural sciences, the fundamental theoretical model is \emph{computational}, meaning that theoretical predictions are made with large computer simulations.
These simulations are usually being asked to handle large ranges of length scales and time scales, so they are generally getting larger and more computationally demanding each year.
Quantitative comparison of data with simulations usually involves running enormous numbers of simulations \cite{abc}, because simulations have to span ranges of fundamental parameters and initial conditions and so on.
Thus these simulation requirements are getting very large for contemporary research projects.
For example, in cosmology, contemporary experiments could easily consume the total computing capacity of the United States.
Hence we turn to emulators---trained ML regressions that have learned the input-output relationship of a simulation, or the relationship between an inexpensive low-resolution simulation and an expensive high-resolution experiment.
Emulators can make computationally impossible projects possible.

Recall that because these emulators are ML methods, they generally have uninterpretable internal parameters and weights, and they generally have been validated by comparison with held-out training data.
Like all high-capacity ML methods, they generally are vulnerable to adversarial attacks \cite{adversarial1}.

Now imagine that two cosmology experiments proceed to do enormous emulator-based inferences on large data sets.
Imagine that these inferences are so large that it would have been exceedingly expensive to have done them with the original first-principles simulations instead of the emulators.
Experiment A finds cosmological parameters very much in line with our expectations going in.
Experiment B finds something extremely surprising about the mass matrix for the neutrinos, in conflict with other measurements and our expectations.
Is it possible that the anomaly discovered by Experiment B isn't real; it just comes from something very slightly wrong or off in the trained ML emulator?

By construction, both experiments are extremely expensive to re-analyze with first-principles simulations.
Which one would we fund for reproduction?
The motivation to re-analyze Experiment B is far, far higher than the motivation to re-analyze Experiment A.
That is a classic example of \emph{confirmation bias}.

Since the money involved is astronomical, there is no simple fix for this unless we plan to re-analyze all experiments using first-principles simulations, no matter what they find.
If that is what's required to avoid the bias, then we should just analyze everything with simulations in the first place, and never use the emulators at any stage.

\section{When is machine learning good for natural science?}\label{sec:good}
In \secref{sec:intro} we re-phrased the question in the title as
``Where, in the natural sciences, can you use a model you don't understand?''
The answer is: You can use it for the parts of your scientific project or experiment or analysis that \emph{you don't need to understand}.
We have colleagues who believe that there is no such part.
We disagree: For example in astronomy, we use charge-coupled devices to make extremely precise measurements of stars, without knowing in detail the solid-state physics that makes them efficient, linear detectors of photons.
There are many parts of a scientific project that must work well, but which do not need to be carefully controlled or understood (in advance, anyway).

Here we give a few examples.
These are not intended to be exhaustive; they are just examples of places within a scientific project ML can be used to the benefit---and not detriment---of the natural-science goals.

\paragraph{Real-time execution and operations:}
In many scientific projects, experiments are adaptive to real-time outcomes, or data are taken selectively, or limited resources are assigned to particular targets.
When bandwidths are high, or experimental timescales are short, sometimes the decisions that need to be made in real time cannot be made with the first-principles decision-making that the investigator would like.
An example mentioned above is the selection of events for storage at the \textsl{LHC Atlas} Experiment \cite{atlas}.
An example from astronomy is the upcoming \textsl{Rubin Observatory LSST} project \cite{rubin}, which will generate some $10^5$ events of possible interest for follow-up observing every night \cite{lsst_events}.
These decisions can be made with very fast classifiers, trained on well-studied training data or simulations \cite{lsst_broker, atlas_trigger}.

Is it unsafe to operate a project using ML decision-making systems?
Even if these systems do not have to be extremely well understood in real time, but they do need to be \emph{versioned} and preserved for study later.
Statistical projects making use of the data from systems like this will need to understand the statistical biases created by the ML selection procedures.
This can be done after the fact, provided that the decision-making system is preserved in a precisely reproducible state, such that it can be run offline on counterfactual, simulated, and subsequent data.

\paragraph{Discoveries of outliers and rare objects:}
As mentioned in \secref{sec:why}, ML is well suited to outlier detection.
Since ML methods are expressive, and trained to fit the data, new data that \emph{aren't} well described by a trained ML model are possibly interesting.
Outlier-detection systems can be used to identify time intervals in which the equipment is malfunctioning.
They can also be used to identify objects that are rare or unusual.
Many discoveries in astrophysics are discoveries of new kinds of objects, originally found as data outliers \cite{quasars, voorwerp}.
So far, ML has not been involved in any big discoveries in astronomy, but we expect this to be a productive avenue for ML in the natural sciences and astronomy in particular.

\paragraph{Foreground, background, and confounder models:}
We mentioned in \secref{sec:philosophy} that a natural-science theory is a combination of a fundamental, latent theory with a (usually complicated) auxiliary observation model that explains the details of the observations.
The simplest part of this observation model is the model of the backgrounds, foregrounds, and confounders.
For example, any maps of the cosmic microwave background (by the ESA \textsl{Planck} spacecraft \cite{planck_maps}, for example) will have emission from the intervening Milky Way and from radio galaxies, adding to, and distorting the maps.
When the goal is to understand the cosmic microwave background, the investigator often doesn't need to understand these foregrounds \emph{physically}; an \emph{effective model} is sufficient.
And indeed, all the bleeding-edge methods for this problem currently are ML methods \cite{cmb_foregrounds}.
It doesn't matter to the questions of physical cosmology what those foregrounds \emph{mean}; and indeed with ML-based foreground removal, cosmic microwave background experiments have delivered the most precise measurements of the parameters of the cosmological model with sub-percent accuracy \cite{planck_parameters}.
For foregrounds and backgrounds, understanding is not always necessary.

An even stronger statement can be made here, however, in the realm of causal inference:
Sometimes the goal is to argue that an effect is being caused by some particular cause, and there are confounding causal inputs that might be distorting or influencing the data.
The more expressive the model used to model the confounders, the stronger the conclusions that can be made about the causation \cite{causal_inference}.
That is, the huge capacity of ML methods can made a causal claim \emph{very conservative}, and hence very robust.
One way to see this is that if the confounder model has been given every chance it possibly can have to model the effect of interest, and it cannot, then it is a stronger conclusion that the effect is caused by the cause of interest.
This is the strategy in contemporary causal-inference projects, which make use of ML methods for these reasons \cite{bart}.

A key place for this kind of causal inference has been in instrument calibration.
Data taken by an instrument have signals imprinted from the physical (or biological or chemical) processes of interest, and also from the details of the measurement hardware, such as sensitivity and bias variations, and time-dependent distortions of internal mappings.
In the end, if the results of the investigation are to have the instrument calibrated out, it makes sense to give the instrument model a lot of expressive power, and judge that model in terms of its ability to explain the data.
Hence, instrument models and calibration are important places for ML methods, and places where the introduction of ML methods can made scientific projects more conservative and more accurate.
One very early example of this in astronomy was the calibration of the \textsl{Sloan Digital Sky Survey} imaging data \cite{ubercalibration}.

\section{Discussion}\label{sec:discussion}

We hope we have clearly answered the question in the title with the response ``Both!''
Machine learning (ML) is a reality for the natural sciences:
We need it for speed and scale in many contexts in many contemporary scientific projects, where instrument bandwidths, data volumes, assay parallelization, experimental automation, and scientific ambitions are all growing.
It is also the case that PhD students and practitioners in the natural sciences want to---or even need to---learn ML practices to have useful and transferable skills.
ML is here, and it is here to stay.

That said, there is a certain amount of hype in this area, and we are seeing scientists ``move fast and break things'' all over the place.
If anything, this \documentname{} represents a call to slow down and think more.
ML tools are incredibly powerful, if we put them to use in the right places in the natural sciences.
In particular, the biases we highlight in \secref{sec:bad}

There is a famous article by Wigner about ``the unreasonable effectiveness of mathematics in the natural sciences'' \cite{wigner}.
Mathematics has been pursued (in many cases) for abstract reasons of beauty and technical challenge, but it has ended up creating a language for many of the natural sciences.
Similarly, we could coin a phrase about \emph{the unreasonable effectiveness of machine learning in the natural sciences}.
It's unreasonable because ML methods were developed, engineered, and optimized to solve problems for commerce.
The most remarkable ML systems have been driven by critical industrial questions such as:
Which advertisement should I display on this web search result?
What direction should I steer this automobile?
How could I make a chair that looks like an avocado?
And which online videos contain kittens?
In some sense it is remarkable that there is \emph{any} overlap between these technologies and the things we need in scientific domains.

But that said, the industrial successes of ML can also be misleading.
The fact that big ML models can solve countless problems in business does not mean that all areas of the natural sciences will be helped by the introduction of ML.
You do not have to \emph{understand} your customers (nor their videos) in order to make plenty of revenue off of them; in the natural sciences, it is understanding, not revenue, that we seek.

Finally, we make a philosophical comment:
In physics---and in most of the natural sciences we believe---the same phenomena can be explained with many qualitatively different theories \cite{philosphy_of_physics}; mathematical theories are not usually unique.
In the same way (as we noted above), ML models contain massive degeneracies, and even different architectures can lead to near-identical predictions after training on the same training data.
Does this lead to a philosophical connection between ML and the natural sciences?
We have argued elsewhere \cite{hogg} that the degeneracies in the natural sciences undermine \emph{realism} or the belief in the literal truth of the latent theories or objects of physics.
There is an argument to be made that maybe the natural sciences could be better off if they moved at least slightly closer to the ML points of view on ontology and epistemology.
In the long run, the data are more stable than the fundamental theories of physics:
Newtonian gravity was replaced with general relativity, and we expect general relativity to be replaced by some kind of quantum gravitational theory.
At each change, the ontology changes, while the data are stable.
We are not recommending anything (yet), we are merely noting that a robust philosophy of natural science might be more positively oriented towards the data than current standard practice.

\paragraph{Acknowledgements:}
It is a pleasure to thank
  Gaby Contardo (SISSA),
  Jennifer Hill (NYU),
  Adrian Price-Whelan (Flatiron),
  Hans-Walter Rix (MPIA),
  Sam Roweis (deceased),
  Bernhard Sch\"olkopf (MPI-IS), and
  Soledad Villar (JHU)
for conversations over the years related to these arguments, and
  Vedant Chandra (Harvard)
for discussions of the bias in regression outputs.

{\raggedright\footnotesize
\bibliography{ml_in_astro}
\bibliographystyle{plain}
}

\clearpage\appendix
\section{Population-level biases in regression outputs}\label{app:toy}
A machine-learning (ML) regression learns the best (that it can find) value of a list or vector $\hat{\theta}$ of parameters (weights and thresholds, for example) of an expressive function $f(x;\theta)$ that takes as input a feature list or vector $x$ and outputs a label estimate $y$.
The parameters $\hat{\theta}$ are learned from a training set of labeled data, which is a set of $(x, y)$ pairs.
The learned function $f(x;\hat{\theta})$ delivers a predicted or estimated label $\hat{y}$ for any data point with a complete set of features $x$.
Thus a particular estimated label $\hat{y}_\ast$ for a particular data point with features $x_\ast$ obtains information \emph{both} from the features $x_\ast$ and \emph{also} from the features and labels of all the data points in the training set used to set the parameters $\hat{\theta}$.
In applications in which regression-estimated labels are used, it matters \emph{how much information} is coming from the individual data-point's features and how much from the original training set.

To illustrate these ideas, we construct a toy data analysis.
This toy will make a general point, but it has a decidedly astronomical feel to it.
The complete toy data set is generated by a process with the algorithm below.
This algorithm makes references to objects $\xi_\eta$ and $\xi_\zeta$ which are generated prior to the start.
\begin{hoggnumerate}
    \item A floating-point value of a known parameter called ``guiding radius'' $r$ is generated in the range $0<r<14$.
    \item A floating-point value of a latent parameter called ``age'' $\eta$ is generated from a Gaussian with mean $14 - r$ and variance 4.
    \item The point is discarded if the age is outside the range $0<\eta<14$. If the point is not discarded:
    \item A $K$-dimensional latent vector $\zeta$ with $K=14$ is generated from a Gaussian with zero mean and variance 1.
    \item A $M$-dimensional latent vector $\xi$ with $M=110$ is created by $\xi = \xi_\eta\,\eta + \xi_\zeta\cdot\zeta$, where $\xi_\eta$ is a $M$-vector and $\xi_\zeta$ is a $M\times K$ matrix.
    The elements of $\xi_\eta$ and $\xi_\zeta$ were drawn (before the start) from Gaussians with zero mean and variances $0.01/K$ and $1/K$ respectively.
    \item A rectified latent vector $\tilde{\xi}$ is created by taking $1 + \xi$ and rectifying all pixels to lie in the range $[0, 1]$.
    \item A $M$-dimensional feature vector $x$ called ``the data'' is generated from a Gaussian with mean $\tilde{\xi}$ and variance $0.0025$.
    \item A label $y$ called ``measured age'' is generated from a Gaussian with mean $\eta$ and variance 1.
\end{hoggnumerate}
The data generated this way are shown in the top panels of \figref{fig:regression}.
The data contain a non-trivial relationship between label $y$ (measured age) $y$ and known parameter $r$ (guiding radius).
This relationship is shown in \figref{fig:regression}.
It isn't precisely linear because of the removal of data points with latent ages $\eta$ outside the range $0<\eta<14$.

We construct a three-layer multilayer perceptron (MLP) (using the scikit-learn implementation) with layer sizes of 64, 32, and 16 neurons.
The model is trained on a training set of 4096 data points with features $x$ and labels $y$.
The trained model takes as input features $x$ and outputs estimated labels $\hat{y}$.
The model is validated on a validation set of 2048 data points with features $x$ and labels $y$.
The test set is a much larger set of $10^5$ data points with features $x$ and no labels.
The model is used to make estimated labels $\hat{y}$ in the full test set.
The validation and test labels are shown in the bottom panels of \figref{fig:regression}.
Importantly, \emph{the training, validation, and test samples are drawn from exactly the same process}, with the same distribution in all properties.
\begin{figure}[p!]
\includegraphics[width=0.49\textwidth]{notebooks/data_examples.png}
\includegraphics[width=0.49\textwidth]{notebooks/training_data.png} \\
\includegraphics[width=0.49\textwidth]{notebooks/validation.png}
\includegraphics[width=0.49\textwidth]{notebooks/test_data_results.png}
\caption{\sffamily%
Visualization of the toy regression. \textsl{Top-left:} Random examples of the data vectors $x$, which are one-dimensional images generated from a linear model plus a nonlinearity created by two rectifications. Details of the data generation are given in the text. Each example $x$ is labeled on the right side by the value of its label $y$. \textsl{Top-right:} The training-set labels $y$, plotted against the known parameter $r$, which is not used in the regression (only the vectors $x$ are used). Also shown are solid red circles showing the true mean relationship between $y$ and $r$ in the toy data. Open black squares show the empirical mean relationship measured in bins in the training-set data. \textsl{Bottom-left:} Validation of the trained regression in the validation set, showing that the label estimates $\hat{y}$ are noisy (as expected given the problem set-up) but not strongly biased. \textsl{Bottom-right:} The regression estimates $\hat{y}$ in the very large test set, plotted against the known parameter $r$. Also shown are the same solid red circles and open black squares as in the top-right plot. Black X-shaped symbols show the mean relationship between $\hat{y}$ and $r$. The relationship shown by the Xs is very precise but biased far away from the true relationship, unlike the relationship shown by the open squares (measured in the training set alone).\label{fig:regression}}
\end{figure}

The regression-estimated labels $\hat{y}$ are related to the measured labels $y$ in the held-out validation set with a linear relationship with unit slope and zero intercept.
The comparison to the validation data (bottom-left panel of \figref{fig:regression}) shows that they are not precisely estimated, but there is no evidence for strong bias at the individual-object level.

The true relationship between label $y$ (measured age) and known parameter $r$ (guiding radius) is estimated in a large generated data set by taking means of $y$ in bins of $r$.
This relationship is shown in \figref{fig:regression} with solid red circles.
The empirical relationship between label $y$ and known parameter $r$ in the training-set data is shown in \figref{fig:regression} with open black squares.
These two relationships are statistically consistent, as expected.
The empirical relationship in the test-set data between the estimated label $\hat{y}$ and the known parameter $r$ in the test-set data is shown in \figref{fig:regression} with black X-shaped symbols.
The test-set relationship is strongly biased, deviating from the true relationship at the edges of the $r$ range.

HOGG: TAKE A PARAGRAPH TO EXPLAIN THIS CLAIM: Given the large size of the test set, the precision of the result is high: This bias in the estimated relationship is larger than 30-sigma in the worst data points.

HOGG: COMMENT OR SHOW that the bias goes down as the information in the features goes up (relative to the information in the training-set distribution (the prior)).

In this case, what should the investigator do---the investigator who wants to know the relationship between $y$ (age) and $r$ (guiding radius)?
The investigator should \emph{just use the training set}.
Transferring labels to the test set did not help, even though the test set is far larger than the training set.
The test-set results are very precise (error bars on the X-shaped symbols cannot be visibly shown in \figref{fig:regression}), but they are very wrong.
The training-set results are less precise but unbiased; they are much closer to the truth.

Indeed, none of the issues here come from out-of-distribution problems or distribution shifts.
All three data sets (train, validate, and test) are drawn from an identical parent population.
When distribution shifts are involved (usually the labeled data are the best data, or the earliest, or the easiest to obtain), these kinds of problems only get worse.

Related to that: The most interesting aspect of this problem is that there is no \emph{wrong} bias:
The bias introduced from the training set is a correct bias; the training set represents a prior that accords with our beliefs about the test set.
And yet the population-level inference is biased.
The averaging of points shown by the X-shaped symbols in \figref{fig:regression} is a simple misuse of the regression outputs; that point is trivial.
However, it is a misuse that is occurring repeatedly in astrophysics with catalogs generated by ML regressions HOGG CITE.

All code (including the data-generating code) used in this toy are available under an open-source license at \url{https://github.com/davidwhogg/BadForScience}.

\end{document}
